{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import bson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sabinogs/projetos/tcc/reviews/dump/test/reviews.bson', 'rb') as b:\n",
    "    df = pd.DataFrame(bson.decode_all(b.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios = df[df['requirement_type'].isin(['non-functional requirement', 'other', 'functional requirement'])]\n",
    "df = df[['reviewText','summary','requirement_type']]\n",
    "df['pre'] = pd.Series()\n",
    "comentarios = comentarios[['reviewText','summary','requirement_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_sentences(sentences):\n",
    "    word_without_stop = list()\n",
    "    for sent in sentences:\n",
    "        aux = list()\n",
    "        for word in sent:\n",
    "            if word not in stopwords.words('english'):\n",
    "                aux.append(word)\n",
    "        word_without_stop.append(aux)\n",
    "    \n",
    "    return word_without_stop\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "def lemmatize_sentences(sentences):\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    lw = list()\n",
    "    for sent in sentences:\n",
    "        aux = list()\n",
    "        for word in sent:\n",
    "            aux.append(lemma.lemmatize(word))\n",
    "        lw.append(aux)\n",
    "    \n",
    "    return lw\n",
    "\n",
    "def lemmatize_it(words):\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    return [lemma.lemmatize(word) for word in words]\n",
    "\n",
    "\n",
    "def remove_pon(doc):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(doc)\n",
    "\n",
    "def _stem_it(doc):\n",
    "    stem = PorterStemmer()\n",
    "    return [stem.stem(word) for word in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = comentarios['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it)\n",
    "pre_summary = comentarios['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = pre + pre_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>requirement_type</th>\n",
       "      <th>pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How dissapointing..downloaded and found it doe...</td>\n",
       "      <td>Extreme Dissappointment</td>\n",
       "      <td>non-functional requirement</td>\n",
       "      <td>how dissapoint download and found it doe not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is great for kids my two year old son lov...</td>\n",
       "      <td>great</td>\n",
       "      <td>other</td>\n",
       "      <td>thi is great for kid my two year old son love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Loves the song, so he really couldn't wait to ...</td>\n",
       "      <td>Really cute</td>\n",
       "      <td>non-functional requirement</td>\n",
       "      <td>love the song so he realli couldn t wait to pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My three year old Plays this game the most he ...</td>\n",
       "      <td>Five little monkeys</td>\n",
       "      <td>other</td>\n",
       "      <td>My three year old play thi game the most he lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As a Speech language pathology Assistant I hav...</td>\n",
       "      <td>My patients request this app everytime they se...</td>\n",
       "      <td>other</td>\n",
       "      <td>As a speech languag patholog assist I have a v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  How dissapointing..downloaded and found it doe...   \n",
       "1  This is great for kids my two year old son lov...   \n",
       "2  Loves the song, so he really couldn't wait to ...   \n",
       "3  My three year old Plays this game the most he ...   \n",
       "4  As a Speech language pathology Assistant I hav...   \n",
       "\n",
       "                                             summary  \\\n",
       "0                            Extreme Dissappointment   \n",
       "1                                              great   \n",
       "2                                        Really cute   \n",
       "3                                Five little monkeys   \n",
       "4  My patients request this app everytime they se...   \n",
       "\n",
       "             requirement_type  \\\n",
       "0  non-functional requirement   \n",
       "1                       other   \n",
       "2  non-functional requirement   \n",
       "3                       other   \n",
       "4                       other   \n",
       "\n",
       "                                                 pre  \n",
       "0  how dissapoint download and found it doe not a...  \n",
       "1  thi is great for kid my two year old son love ...  \n",
       "2  love the song so he realli couldn t wait to pl...  \n",
       "3  My three year old play thi game the most he lo...  \n",
       "4  As a speech languag patholog assist I have a v...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comentarios['pre'] = pre\n",
    "comentarios['pre'] = comentarios['pre'].apply(lambda x: ' '.join(x))\n",
    "comentarios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabinogs/.virtualenvs/explore/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "c = comentarios[comentarios['requirement_type'] != 'other']\n",
    "c['pre'] = c['pre'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "Para maior referencia, olhe os links acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = c['pre'].tolist()\n",
    "y = c['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-9c51dc2630d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \"\"\"\n\u001b[1;32m   1582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1012\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    942\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "text_classifier.fit(X_train,y_train)\n",
    "predicted = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "    functional requirement       0.60      0.98      0.74       176\n",
      "non-functional requirement       0.75      0.10      0.17       126\n",
      "\n",
      "                 micro avg       0.61      0.61      0.61       302\n",
      "                 macro avg       0.68      0.54      0.46       302\n",
      "              weighted avg       0.66      0.61      0.50       302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Um Classificador para identificar todas as categorias (Experimento 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                         2295\n",
       "functional requirement         596\n",
       "non-functional requirement     410\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comentarios['requirement_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "    functional requirement       0.00      0.00      0.00       177\n",
      "non-functional requirement       0.00      0.00      0.00       120\n",
      "                     other       0.70      1.00      0.82       694\n",
      "\n",
      "                 micro avg       0.70      0.70      0.70       991\n",
      "                 macro avg       0.23      0.33      0.27       991\n",
      "              weighted avg       0.49      0.70      0.58       991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabinogs/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X = comentarios['pre'].tolist()\n",
    "y = comentarios['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_exp1 = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_exp1.fit(X_train,y_train)\n",
    "predicted_exp1 = text_classifier_exp1.predict(X_test)\n",
    "print(classification_report(y_test, predicted_exp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nessa seção criarei um classificador para cada Requisito\n",
    "\n",
    "\n",
    "\n",
    "Isso se dá pela necessidade de aumentar o desempenho do classificador. Para isso, será criado 3 classificadores que: \n",
    "\n",
    "1. Identificará se um comentário é do tipo \"Funcional\" ou não\n",
    "2. Identificará se um comentário é do tipo \"Não Funcional\" ou não\n",
    "3. Identificará se um comentário é do tipo \"Outro\" ou não\n",
    "\n",
    "\n",
    "Observe o uso do `DataFrame.copy()`. Isso deve-se ao fato de que ao realizar uma atribuição de dataframe `novoDF = velho` copiamos apenas a referência e isso implicará que mudar uma coisa no `novoDF` muda tbm no `velho`. \n",
    "\n",
    "\n",
    "\n",
    "Ref: \n",
    "\n",
    "1. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nao_funcionais = comentarios.copy(deep=True)\n",
    "outros = comentarios.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(documento, requirement_type):\n",
    "    if documento == requirement_type:\n",
    "        return 'sim'\n",
    "    else:\n",
    "        return 'nao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "nao_funcionais['requirement_type'] = nao_funcionais['requirement_type'].apply(\n",
    "    lambda x: transform_dataset(x,'non-functional requirement')\n",
    ")\n",
    "\n",
    "\n",
    "outros['requirement_type'] = outros['requirement_type'].apply(\n",
    "    lambda x: transform_dataset(x,'other')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 2\n",
    "\n",
    "## Dataset e Classificador para FUNCIONAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcionais = comentarios.copy(deep=True)\n",
    "funcionais['requirement_type'] = funcionais['requirement_type'].apply(\n",
    "    lambda x: transform_dataset(x,'functional requirement')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nao    2705\n",
       "sim     596\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcionais['requirement_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.82      1.00      0.90       814\n",
      "         sim       0.00      0.00      0.00       177\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       991\n",
      "   macro avg       0.41      0.50      0.45       991\n",
      "weighted avg       0.67      0.82      0.74       991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabinogs/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_pre_review = funcionais['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = funcionais['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "funcionais['pre'] = pre\n",
    "funcionais['pre'] = funcionais['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_funcionais = funcionais['pre'].tolist()\n",
    "y_funcionais = funcionais['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_funcionais, y_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_funcionais = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_funcionais.fit(X_train, y_train)\n",
    "predicted_funcionais = text_classifier_funcionais.predict(X_test)\n",
    "print(classification_report(y_test, predicted_funcionais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset e Classificador para NAO FUNCIONAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "nao_funcionais = comentarios.copy(deep=True)\n",
    "nao_funcionais['requirement_type'] = nao_funcionais['requirement_type'].apply(\n",
    "    lambda x: transform_dataset(x,'non-functional requirement')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nao    2891\n",
       "sim     410\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nao_funcionais['requirement_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.88      1.00      0.94       871\n",
      "         sim       0.00      0.00      0.00       120\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       991\n",
      "   macro avg       0.44      0.50      0.47       991\n",
      "weighted avg       0.77      0.88      0.82       991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabinogs/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_pre_review = nao_funcionais['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = nao_funcionais['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "nao_funcionais['pre'] = pre\n",
    "nao_funcionais['pre'] = nao_funcionais['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_nao_funcionais = nao_funcionais['pre'].tolist()\n",
    "y_nao_funcionais = nao_funcionais['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nao_funcionais, y_nao_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_nao_funcionais = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_nao_funcionais.fit(X_train, y_train)\n",
    "predicted_nao_funcionais = text_classifier_nao_funcionais.predict(X_test)\n",
    "print(classification_report(y_test, predicted_nao_funcionais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 3\n",
    "\n",
    "## Criando datasets balanceados\n",
    "\n",
    "### Funcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nao    1286\n",
       "sim     596\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_array = funcionais[funcionais['requirement_type'] == 'nao'].index\n",
    "funcionais_to_be_removed = np.random.choice(index_array,int(3*len(index_array)/4))\n",
    "\n",
    "funcionais_balanceado = funcionais.drop(funcionais_to_be_removed)\n",
    "funcionais_balanceado['requirement_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não funcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nao    1344\n",
       "sim     410\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_array = nao_funcionais[nao_funcionais['requirement_type'] == 'nao'].index\n",
    "nao_funcionais_to_be_removed = np.random.choice(index_array,int(3*len(index_array)/4))\n",
    "\n",
    "nao_funcionais_balanceado = nao_funcionais.drop(nao_funcionais_to_be_removed)\n",
    "nao_funcionais_balanceado['requirement_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando os classificadores balanceados\n",
    "\n",
    "### Funcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.70      0.99      0.82       392\n",
      "         sim       0.77      0.06      0.11       173\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       565\n",
      "   macro avg       0.74      0.53      0.47       565\n",
      "weighted avg       0.72      0.71      0.60       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pre_review = funcionais_balanceado['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = funcionais_balanceado['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "funcionais_balanceado['pre'] = pre\n",
    "funcionais_balanceado['pre'] = funcionais_balanceado['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_funcionais = funcionais_balanceado['pre'].tolist()\n",
    "y_funcionais = funcionais_balanceado['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_funcionais, y_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_funcionais_balanceados = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_funcionais_balanceados.fit(X_train, y_train)\n",
    "predicted_funcionais_balanceados = text_classifier_funcionais_balanceados.predict(X_test)\n",
    "print(classification_report(y_test, predicted_funcionais_balanceados))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não funcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.75      1.00      0.86       397\n",
      "         sim       1.00      0.01      0.02       130\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       527\n",
      "   macro avg       0.88      0.50      0.44       527\n",
      "weighted avg       0.82      0.76      0.65       527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pre_review = nao_funcionais_balanceado['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = nao_funcionais_balanceado['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "nao_funcionais_balanceado['pre'] = pre\n",
    "nao_funcionais_balanceado['pre'] = nao_funcionais_balanceado['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_nao_funcionais = nao_funcionais_balanceado['pre'].tolist()\n",
    "y_nao_funcionais = nao_funcionais_balanceado['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nao_funcionais, y_nao_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_nao_funcionais_balanceados = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_nao_funcionais_balanceados.fit(X_train, y_train)\n",
    "predicted_nao_funcionais_balanceados = text_classifier_nao_funcionais_balanceados.predict(X_test)\n",
    "print(classification_report(y_test, predicted_nao_funcionais_balanceados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nao    1290\n",
       "sim     596\n",
       "Name: requirement_type, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_array = funcionais[funcionais['requirement_type'] == 'nao'].index\n",
    "to_be_removed = np.random.choice(index_array,int(3*len(index_array)/4))\n",
    "\n",
    "_f = funcionais.drop(to_be_removed)\n",
    "_f['requirement_type'].value_counts()\n",
    "# len(funcionais[funcionais['requirement_type'] == 'nao'])\n",
    "# len(to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classificador de Requisitos Funcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       [As, a, speech, languag, patholog, assist, I, ...\n",
       "5       [thought, it, would, be, the, book, to, read, ...\n",
       "7       [We, love, these, monkey, s, and, all, the, co...\n",
       "8       [I, found, thi, at, a, perfect, time, sinc, my...\n",
       "9       [My, granddaught, an, 18, month, old, love, th...\n",
       "10      [I, like, the, fact, that, I, can, purchas, mu...\n",
       "11      [I, watch, my, great, grandson, 4, day, a, wee...\n",
       "12      [It, wont, download, my, playlist, offlin, any...\n",
       "13      [I, have, a, kindl, fire, first, gen, and, thi...\n",
       "15      [I, am, so, disappoint, that, I, actual, paid,...\n",
       "16      [No, matter, what, kind, of, music, I, want, t...\n",
       "17      [you, cant, use, thi, unless, you, have, a, pa...\n",
       "18      [is, the, music, I, like, non, commerci, so, I...\n",
       "21      [love, that, I, can, type, ani, song, in, and,...\n",
       "22      [not, worth, the, download, what, a, wast, of,...\n",
       "23      [My, granddaught, realli, love, thi, she, will...\n",
       "25      [onli, issu, is, new, music, doesnt, alway, sh...\n",
       "26      [great, music, I, listen, TO, IT, AT, work, an...\n",
       "27      [I, realli, like, be, abl, to, add, song, to, ...\n",
       "30                                                [remov]\n",
       "31      [excel, app, easi, to, use, on, my, kindl, fir...\n",
       "35      [start, my, day, with, the, daili, vers, and, ...\n",
       "39      [love, the, rhapsodi, app, you, can, stream, a...\n",
       "41      [It, allow, me, to, access, rhapsodi, on, my, ...\n",
       "43      [rhapsodi, is, the, essenti, music, tool, of, ...\n",
       "45      [I, ve, been, a, die, hard, itun, fan, forev, ...\n",
       "48      [went, look, for, a, app, to, listen, to, tune...\n",
       "49      [I, know, thi, app, ha, been, out, for, a, whi...\n",
       "50      [i, give, thi, rate, 1, star, i, download, rha...\n",
       "51      [the, rhapsodi, app, turn, my, android, phone,...\n",
       "                              ...                        \n",
       "3249    [My, husband, realli, like, thi, app, It, is, ...\n",
       "3250    [thi, app, is, fantast, especi, dure, emerg, f...\n",
       "3251    [thi, app, work, veri, well, great, to, listen...\n",
       "3252    [doe, everyth, it, say, scan, all, differ, fre...\n",
       "3253    [I, wa, impress, with, all, it, pick, up, I, a...\n",
       "3256    [I, wa, impress, at, the, ea, of, instal, and,...\n",
       "3260    [doe, everyth, it, say, it, doe, listen, to, y...\n",
       "3261    [like, thi, now, I, don, t, have, to, be, in, ...\n",
       "3263    [I, like, thi, so, far, just, wish, I, could, ...\n",
       "3265    [it, will, be, even, better, a, more, citi, ar...\n",
       "3294    [over, the, year, I, ve, use, sever, differ, s...\n",
       "3295    [As, a, former, scanner, listen, thi, is, a, p...\n",
       "3298    [thi, is, one, of, my, favorit, app, I, can, l...\n",
       "3304    [thi, app, is, ok, for, everyday, listen, but,...\n",
       "3308    [I, live, in, smaller, town, thi, is, the, onl...\n",
       "3309    [If, you, want, a, scanner, that, is, easi, of...\n",
       "3310    [It, work, well, and, there, are, ton, of, opt...\n",
       "3313    [thi, is, a, great, app, that, let, you, liste...\n",
       "3314    [work, great, pick, up, my, area, well, so, I,...\n",
       "3335    [It, doe, work, but, mani, area, have, limit, ...\n",
       "3338    [nice, scanner, occasion, freez, up, though, o...\n",
       "3339    [doe, what, it, say, it, doe, would, not, chan...\n",
       "3340    [No, need, to, buy, an, expens, scanner, when,...\n",
       "3341    [I, alway, know, where, my, love, one, are, an...\n",
       "3348    [never, listen, to, a, scanner, befor, but, no...\n",
       "3349    [just, start, use, thi, app, about, a, week, a...\n",
       "3350    [would, rather, have, info, closer, to, home, ...\n",
       "3351    [there, wa, noth, in, my, area, I, tri, all, o...\n",
       "3352    [I, wa, hope, to, be, abl, to, listen, to, loc...\n",
       "3353    [great, littl, app, to, have, come, in, handi,...\n",
       "Name: reviewText, Length: 1886, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_f['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.69      1.00      0.82       383\n",
      "         sim       0.92      0.07      0.12       183\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       566\n",
      "   macro avg       0.81      0.53      0.47       566\n",
      "weighted avg       0.77      0.70      0.59       566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pre_review = _f['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = _f['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "_f['pre'] = pre\n",
    "_f['pre'] = _f['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_funcionais = _f['pre'].tolist()\n",
    "y_funcionais = _f['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_funcionais, y_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_funcionais = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_funcionais.fit(X_train, y_train)\n",
    "predicted_funcionais = text_classifier_funcionais.predict(X_test)\n",
    "print(classification_report(y_test, predicted_funcionais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nao       0.82      1.00      0.90       814\n",
      "         sim       0.00      0.00      0.00       177\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       991\n",
      "   macro avg       0.41      0.50      0.45       991\n",
      "weighted avg       0.67      0.82      0.74       991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabinogs/.virtualenvs/explore/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_pre_review = funcionais['reviewText'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "_pre_summary = funcionais['summary'].apply(remove_pon).apply(_stem_it).apply(lemmatize_it).apply(remove_stopwords)\n",
    "\n",
    "pre = _pre_review + _pre_summary\n",
    "funcionais['pre'] = pre\n",
    "funcionais['pre'] = funcionais['pre'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_funcionais = funcionais['pre'].tolist()\n",
    "y_funcionais = funcionais['requirement_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_funcionais, y_funcionais, test_size=0.3, random_state=42)\n",
    "\n",
    "text_classifier_funcionais = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf',MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_classifier_funcionais.fit(X_train, y_train)\n",
    "predicted_funcionais = text_classifier_funcionais.predict(X_test)\n",
    "print(classification_report(y_test, predicted_funcionais))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
